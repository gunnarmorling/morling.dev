<!DOCTYPE html>
<html>
<head>
	
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-DD997656SV"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-DD997656SV');
	</script>

	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>A Deep Dive Into Ingesting Debezium Events From Kafka With Flink SQL - Gunnar Morling</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="A Deep Dive Into Ingesting Debezium Events From Kafka With Flink SQL" />
<meta property="og:description" content="
Over the years, I’ve spoken quite a bit about the use cases for processing Debezium data change events with Apache Flink,
such as metadata enrichment, building denormalized data views, and creating data contracts for your CDC streams.
One detail I haven’t covered in depth so far is how to actually ingest Debezium change events from a Kafka topic into Flink,
in particular via Flink SQL.
Several connectors and data formats exist for this, which can make things somewhat confusing at first.
So let’s dive into the different options and the considerations around them!
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.morling.dev/blog/ingesting-debezium-events-from-kafka-with-flink-sql/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2025-04-16T11:25:00+02:00" />
<meta property="article:modified_time" content="2025-04-16T11:25:00+02:00" />

<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="A Deep Dive Into Ingesting Debezium Events From Kafka With Flink SQL"/>
<meta name="twitter:description" content="
Over the years, I’ve spoken quite a bit about the use cases for processing Debezium data change events with Apache Flink,
such as metadata enrichment, building denormalized data views, and creating data contracts for your CDC streams.
One detail I haven’t covered in depth so far is how to actually ingest Debezium change events from a Kafka topic into Flink,
in particular via Flink SQL.
Several connectors and data formats exist for this, which can make things somewhat confusing at first.
So let’s dive into the different options and the considerations around them!
"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet" />
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" />
 
	<link rel="stylesheet" type="text/css" media="screen" href="https://www.morling.dev/css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="https://www.morling.dev/css/main.css" />

	
	<link rel="stylesheet" type="text/css" href="https://www.morling.dev/css/base16.dark.css" />
	
	<link rel="stylesheet" type="text/css" href="https://www.morling.dev/css/morlingdev.css" />
	

	<script>
		const searchUrl = "https:\/\/search-morling-dev.onrender.com\/";
		const apiKey = "ff90d45f4afad3bd914c";
	</script>

	<script src="https://www.morling.dev//js/main.js"></script>
	<script src="https://www.morling.dev//js/medium-zoom.min.js"></script>

	<noscript>
		<style type="text/css">
			.club { display:none; }
		</style>
	</noscript>
</head>

<body>
	<div class="container wrapper post">
		<div class="header desktop">

	<div class="row">
		<div class="header-image-container">
			<img class="header-image" src="/images/gunnar_morling.jpg" alt="Gunnar Morling">
		</div>
		<div class="fill">
			<h1 class="site-title"><a href="https://www.morling.dev/">Gunnar Morling</a></h1>
			<div class="site-description"><h2>Random Musings on All Things Software Engineering</h2></div>

			<nav class="row pre-nav">
				<div class="pull-right">
					<ul class="flat"><li>
							<a href="/blog/index.xml" title="RSS FEED">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#rss"/>
								</svg>
							</a>
						</li><li>
							<a href="https://github.com/gunnarmorling" title="GitHub">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#github"/>
								</svg>
							</a>
						</li><li>
							<a href="https://bsky.app/profile/gunnarmorling.dev" title="Bluesky">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#cloud"/>
								</svg>
							</a>
						</li><li>
							<a href="https://twitter.com/gunnarmorling" title="Twitter">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#twitter"/>
								</svg>
							</a>
						</li><li>
							<a href="https://www.linkedin.com/in/gunnar-morling/" title="LinkedIn">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#linkedin"/>
								</svg>
							</a>
						</li><li>
							<a href="https://mastodon.online/@gunnarmorling" title="Mastodon">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#message-square"/>
								</svg>
							</a>
						</li></ul>
				</div>
			</nav>
			<nav class="row nav">
				<div>
					<ul class="flat">
						
						<li>
							<a href="/">Blog</a>
						</li>
						
						<li>
							<a href="/projects/">Projects</a>
						</li>
						
						<li>
							<a href="/conferences/">Conferences</a>
						</li>
						
						<li>
							<a href="/podcasts/">Podcasts</a>
						</li>
						
						<li>
							<a href="/about/">About</a>
						</li>
						
					</ul>
				</div>
				<div class="pull-right">
					<div class="club">
						<form id="myForm">
							<input type="text" id="inputSearch" name="q" placeholder="Search..." onfocus="warmUp(this)">
							<button type="submit" id="buttonSubmitSearch" style="line-height: normal;"><i id="iconSearch" class="fa fa-search"></i></button>
						</form>
					</div>
				</div>
			</nav>
		</div>
	</div>
</div>

<div class="header mobile">

	<div class="row">
		<div class="header-image-container">
			<img class="header-image" src="/images/gunnar_morling.jpg" alt="Gunnar Morling">
		</div>
		<div class="fill">
			<h1 class="site-title"><a href="https://www.morling.dev/">Gunnar Morling</a></h1>
			<div class="site-description"><h2>Random Musings on All Things Software Engineering</h2></div>
		</div>
	</div>
	<div>
		<div>
			<nav class="row pre-nav">
				<div class="pull-right">
					<ul class="flat"><li>
							<a href="/blog/index.xml" title="RSS FEED">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#rss"/>
								</svg>
							</a>
						</li><li>
							<a href="https://github.com/gunnarmorling" title="GitHub">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#github"/>
								</svg>
							</a>
						</li><li>
							<a href="https://bsky.app/profile/gunnarmorling.dev" title="Bluesky">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#cloud"/>
								</svg>
							</a>
						</li><li>
							<a href="https://twitter.com/gunnarmorling" title="Twitter">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#twitter"/>
								</svg>
							</a>
						</li><li>
							<a href="https://www.linkedin.com/in/gunnar-morling/" title="LinkedIn">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#linkedin"/>
								</svg>
							</a>
						</li><li>
							<a href="https://mastodon.online/@gunnarmorling" title="Mastodon">
								<svg width="17" height="17" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
									<use xlink:href="/svg/feather-sprite.svg#message-square"/>
								</svg>
							</a>
						</li></ul>
				</div>
			</nav>
			<nav class="row nav">
				<div>
					<ul class="flat">
						
						<li>
							<a href="/">Blog</a>
						</li>
						
						<li>
							<a href="/projects/">Projects</a>
						</li>
						
						<li>
							<a href="/conferences/">Conferences</a>
						</li>
						
						<li>
							<a href="/podcasts/">Podcasts</a>
						</li>
						
						<li>
							<a href="/about/">About</a>
						</li>
						
					</ul>
				</div>
				<div class="pull-right">
					<div class="club">
						<form id="myFormMobile">
							<input type="text" id="inputSearchMobile" name="q" placeholder="Search..." onfocus="warmUp(this)">
							<button type="submit" id="buttonSubmitSearchMobile" style="line-height: normal;"><i id="iconSearchMobile" class="fa fa-search"></i></button>
						</form>
					</div>
				</div>
			</nav>
		</div>
	</div>
</div>

<script type="text/javascript">
	window.addEventListener( "load", function () {
		const urlParams = new URLSearchParams(window.location.search);

		


		const form = document.getElementById( "myForm" );

		form.addEventListener("submit", function (event) {
			event.preventDefault();
			sendData(new FormData(form));
		});

		const formMobile = document.getElementById( "myFormMobile" );

		formMobile.addEventListener("submit", function (event) {
			event.preventDefault();
			sendData(new FormData(formMobile));
		});
	});
</script>


		<div id = "main-content">
			<div class="post-header">
				<h1 class="title">A Deep Dive Into Ingesting Debezium Events From Kafka With Flink SQL</h1>
				<div class="meta">Posted at Apr 16, 2025</div>
			</div>

			<div class="markdown">
				<div class="paragraph">
<p>Over the years, I’ve spoken quite a bit about the use cases for processing <a href="https://2023.javazone.no/program/355869fa-5aa0-43a7-abd2-7c5250e10bcd">Debezium data change events with Apache Flink</a>,
such as metadata enrichment, building denormalized data views, and creating data contracts for your CDC streams.
One detail I haven’t covered in depth so far is how to actually ingest Debezium change events from a Kafka topic into Flink,
in particular via Flink SQL.
Several connectors and data formats exist for this, which can make things somewhat confusing at first.
So let’s dive into the different options and the considerations around them!</p>
</div>
<div class="sect1">
<h2 id="_flink_sql_connectors_for_apache_kafka">Flink SQL Connectors for Apache Kafka</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For processing events from a Kafka topic using Flink SQL (or the Flink Table API, which essentially offers a programmatic counterpart to SQL), there are two connectors:
The <a href="https://nightlies.apache.org/flink/flink-docs-release-2.0/docs/connectors/table/kafka/">Apache Kafka SQL connector</a> and the <a href="https://nightlies.apache.org/flink/flink-docs-release-2.0/docs/connectors/table/upsert-kafka/">Upsert Kafka SQL Connector</a>.</p>
</div>
<div class="paragraph">
<p>Both connectors can be used as a source connector—​reading data from a Kafka topic—​and as a sink connector, for writing data to a Kafka topic.
There’s support for different data formats such as JSON and Apache Avro,
the latter with a schema registry such as the <a href="https://github.com/confluentinc/schema-registry">Confluent schema registry</a>,
or API-compatible implementations like <a href="https://www.apicur.io/registry/">Apicurio</a>.
The Apache Kafka SQL Connector also supports Debezium-specific JSON and Avro formats.</p>
</div>
<div class="paragraph">
<p>The combination of connector and format defines the exact semantics,
in particular whether the ingested Debezium events are processed as an append-only stream,
or as a changelog stream, building and incrementally updating materialized views of the source tables based on the incoming <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> events
(<a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/concepts/dynamic_tables/">Dynamic Tables</a> in Flink SQL terminology).</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_apache_kafka_sql_connector_in_append_only_mode">The Apache Kafka SQL Connector in Append-Only Mode</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When using the Apache Kafka SQL Connector with the <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/json/">JSON format</a>,
no Debezium-specific semantics are applied:
The Kafka topic with the Debezium events is interpreted as an append-only log of independent events.
The same is the case when using the <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/avro-confluent/">Confluent Avro</a> format instead of JSON.</p>
</div>
<div class="paragraph">
<p>The schema of the table must be exactly modeled after Debezium’s <a href="https://debezium.io/documentation/reference/stable/connectors/postgresql#postgresql-change-events-value">data event structure</a>,
including all the fields of both message key (representing the record’s primary key) and message value (the change event):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">authors_append_only_source</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">BIGINT</span> <span class="k">NOT</span> <span class="k">NULL</span><span class="p">,</span> <i class="conum" data-value="1"></i><b>(1)</b>
  <span class="k">before</span> <span class="k">ROW</span><span class="p">(</span>  <i class="conum" data-value="2"></i><b>(2)</b>
    <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
    <span class="n">first_name</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">last_name</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">biography</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">registered</span> <span class="nb">BIGINT</span>
  <span class="p">),</span>
  <span class="k">after</span> <span class="k">ROW</span><span class="p">(</span>
    <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
    <span class="n">first_name</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">last_name</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">biography</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">registered</span> <span class="nb">BIGINT</span>
  <span class="p">),</span>
  <span class="k">source</span> <span class="k">ROW</span><span class="p">(</span>
    <span class="k">version</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">connector</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">name</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">ts_ms</span> <span class="nb">BIGINT</span><span class="p">,</span>
    <span class="n">snapshot</span> <span class="nb">BOOLEAN</span><span class="p">,</span>
    <span class="n">db</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">sequence</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="k">table</span> <span class="n">STRING</span><span class="p">,</span>
    <span class="n">txid</span> <span class="nb">BIGINT</span><span class="p">,</span>
    <span class="n">lsn</span> <span class="nb">BIGINT</span><span class="p">,</span>
    <span class="n">xmin</span> <span class="nb">BIGINT</span>
  <span class="p">),</span>
  <span class="n">op</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">ts_ms</span> <span class="nb">BIGINT</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">&#39;connector&#39;</span> <span class="o">=</span> <span class="s1">&#39;kafka&#39;</span><span class="p">,</span>
  <span class="s1">&#39;topic&#39;</span> <span class="o">=</span> <span class="s1">&#39;dbserver1.inventory.authors&#39;</span><span class="p">,</span>
  <span class="s1">&#39;properties.bootstrap.servers&#39;</span> <span class="o">=</span> <span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span>
  <span class="s1">&#39;scan.startup.mode&#39;</span> <span class="o">=</span> <span class="s1">&#39;earliest-offset&#39;</span><span class="p">,</span> <i class="conum" data-value="3"></i><b>(3)</b>
  <span class="s1">&#39;key.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;json&#39;</span><span class="p">,</span> <i class="conum" data-value="4"></i><b>(4)</b>
  <span class="s1">&#39;key.fields&#39;</span> <span class="o">=</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span>
  <span class="s1">&#39;value.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;json&#39;</span><span class="p">,</span> <i class="conum" data-value="5"></i><b>(5)</b>
  <span class="s1">&#39;value.fields-include&#39;</span> <span class="o">=</span> <span class="s1">&#39;EXCEPT_KEY&#39;</span>
<span class="p">);</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tbody><tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>id</code> field maps to the key of incoming Kafka messages</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>before</code>, <code>after</code>, <code>source</code>, <code>op</code>, and <code>ts_ms</code> fields map to the value of incoming Kafka messages</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Start reading from the earliest offset of the topic</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Use JSON as the format for Kafka keys, with the <code>id</code> field being part of the key</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Use JSON as the format for Kafka values, excluding the key fields (<code>id</code> in this case)</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>When taking a look at the type of the events in the Flink source table—​for instance by setting the <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sqlclient/#sql-client-execution-result-mode">result mode</a> to <code>changelog</code> when querying the table in the Flink SQL client—​you’ll see that all the events are insertions (first <code>op</code> column in the listing below),
no matter what their change event type is from a Debezium perspective (second <code>op</code> column):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre><span class="o">|</span> <span class="n">op</span> <span class="o">|</span>   <span class="n">id</span> <span class="o">|</span>                         <span class="k">before</span> <span class="o">|</span>                          <span class="k">after</span> <span class="o">|</span>                         <span class="k">source</span> <span class="o">|</span> <span class="n">op</span> <span class="o">|</span>         <span class="n">ts_ms</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">----+------+--------------------------------+--------------------------------+--------------------------------+ ---+---------------+</span>
<span class="o">|</span> <span class="o">+</span><span class="n">I</span> <span class="o">|</span> <span class="mi">1001</span> <span class="o">|</span>                         <span class="o">&lt;</span><span class="k">NULL</span><span class="o">&gt;</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1001</span><span class="p">,</span> <span class="n">John</span><span class="p">,</span> <span class="n">Stenton</span><span class="p">,</span> <span class="n">ZbJa0</span><span class="p">...</span> <span class="o">|</span> <span class="p">(</span><span class="mi">3</span><span class="p">.</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="k">Final</span><span class="p">,</span> <span class="n">postgresql</span><span class="p">,</span> <span class="n">d</span><span class="p">...</span> <span class="o">|</span>  <span class="n">r</span> <span class="o">|</span> <span class="mi">1744296502685</span> <span class="o">|</span>
<span class="o">|</span> <span class="o">+</span><span class="n">I</span> <span class="o">|</span> <span class="mi">1008</span> <span class="o">|</span>                         <span class="o">&lt;</span><span class="k">NULL</span><span class="o">&gt;</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1009</span><span class="p">,</span> <span class="n">John</span><span class="p">,</span> <span class="n">Thomas</span><span class="p">,</span> <span class="n">ZbJ0du</span><span class="p">...</span> <span class="o">|</span> <span class="p">(</span><span class="mi">3</span><span class="p">.</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="k">Final</span><span class="p">,</span> <span class="n">postgresql</span><span class="p">,</span> <span class="n">d</span><span class="p">...</span> <span class="o">|</span>  <span class="k">c</span> <span class="o">|</span> <span class="mi">1744360987874</span> <span class="o">|</span>
<span class="o">|</span> <span class="o">+</span><span class="n">I</span> <span class="o">|</span> <span class="mi">1009</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1009</span><span class="p">,</span> <span class="n">John</span><span class="p">,</span> <span class="n">Thomas</span><span class="p">,</span> <span class="n">ZbJ0du</span><span class="p">...</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1009</span><span class="p">,</span> <span class="n">John</span><span class="p">,</span> <span class="n">Beck</span><span class="p">,</span> <span class="n">ZbJ0duaf</span><span class="p">...</span> <span class="o">|</span> <span class="p">(</span><span class="mi">3</span><span class="p">.</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="k">Final</span><span class="p">,</span> <span class="n">postgresql</span><span class="p">,</span> <span class="n">d</span><span class="p">...</span> <span class="o">|</span>  <span class="n">u</span> <span class="o">|</span> <span class="mi">1744626041413</span> <span class="o">|</span>
<span class="o">|</span> <span class="o">+</span><span class="n">I</span> <span class="o">|</span> <span class="mi">1008</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1009</span><span class="p">,</span> <span class="n">John</span><span class="p">,</span> <span class="n">Beck</span><span class="p">,</span> <span class="n">ZbJ0duaf</span><span class="p">...</span> <span class="o">|</span>                         <span class="o">&lt;</span><span class="k">NULL</span><span class="o">&gt;</span> <span class="o">|</span> <span class="p">(</span><span class="mi">3</span><span class="p">.</span><span class="mi">1</span><span class="p">.</span><span class="mi">0</span><span class="p">.</span><span class="k">Final</span><span class="p">,</span> <span class="n">postgresql</span><span class="p">,</span> <span class="n">d</span><span class="p">...</span> <span class="o">|</span>  <span class="n">d</span> <span class="o">|</span> <span class="mi">1744627927160</span> <span class="o">|</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>For writing (potentially processed) change events back into an output topic,
another table can be created with exactly the same schema and configuration,
only that you’d adjust the topic name accordingly and omit the <code>scan.startup.mode</code> option.
The mapping of the key is required for both source and sink table in order to ensure that the partitioning,
and thus the ordering, of the Debezium events on the output topic is the same as on the input topic.</p>
</div>
<div class="paragraph">
<p><em>When to use it:</em> The Apache Kafka SQL Connector in append-only mode is a great choice when you want to operate on a &#34;raw&#34; stream of Debezium data change events, without applying any changelog or upsert semantics.
It comes in handy for applying transformations such as adjusting date formats or filtering events based on specific field values.
In that sense, this is similar to using the Flink DataStream API on a change event stream, only that you are using SQL rather than Java for your processing logic.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_apache_kafka_sql_connector_as_a_changelog_source">The Apache Kafka SQL Connector As a Changelog Source</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Besides the append-only mode, the Apache Kafka SQL Connector also supports <a href="https://archive.fosdem.org/2023/schedule/event/fast_data_cdc_apache_flink/attachments/slides/5563/export/events/attachments/fast_data_cdc_apache_flink/slides/5563/Apache_Flink_CDC_Slides.pdf">changelog semantics</a> via the <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/debezium/">Debezium data format</a>.
Both JSON (by specifying <code>debezium-json</code> as the value format of your table) and Avro with a registry (via <code>debezium-avro-confluent</code>) are supported.
The <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> events ingested from the Kafka topic are used by the Flink SQL engine to incrementally re-compute the corresponding dynamic table, as well as any continuous queries you are running against it.
If you query a changelog-based source table, the result set always represents the current state of that table,
updated in realtime whenever a new Debezium event comes in.</p>
</div>
<div class="paragraph">
<p>The table schema looks quite a bit different than before.
Instead of modeling the entire Debezium envelope structure, only the actual table schema
(i.e. the contents of the <code>before</code> and <code>after</code> sections) needs to be specified:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">authors_changelog_source</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="n">first_name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">last_name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">biography</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">registered</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="k">NOT</span> <span class="n">ENFORCED</span> <i class="conum" data-value="1"></i><b>(1)</b>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">&#39;connector&#39;</span> <span class="o">=</span> <span class="s1">&#39;kafka&#39;</span><span class="p">,</span>
  <span class="s1">&#39;topic&#39;</span> <span class="o">=</span> <span class="s1">&#39;dbserver1.inventory.authors&#39;</span><span class="p">,</span>
  <span class="s1">&#39;properties.bootstrap.servers&#39;</span> <span class="o">=</span> <span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span>
  <span class="s1">&#39;scan.startup.mode&#39;</span> <span class="o">=</span> <span class="s1">&#39;earliest-offset&#39;</span><span class="p">,</span>
  <span class="s1">&#39;value.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;debezium-json&#39;</span> <i class="conum" data-value="2"></i><b>(2)</b>
<span class="p">);</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tbody><tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>While not strictly needed here, a primary key definition—in conjunction with setting the job-level configuration <code>table.exec.source.cdc-events-duplicate</code> to <code>true</code>—ensures that duplicates are discarded in case Debezium events are ingested a second time, for instance after a connector crash</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Using <code>debezium-json</code> as the value format enables changelog semantics for this table</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>When querying this table in the Flink SQL client, the operation type reflects the kind of the incoming Debezium event.
Note how update events are broken up into an update-before event (<code>-U</code>, representing the retraction of the old row) and an update-after event (<code>+U</code>, the insertion of the new row) internally by the Flink SQL engine:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="o">+</span><span class="c1">----+------+------------+-----------+-----------+------------------+</span>
<span class="o">|</span> <span class="n">op</span> <span class="o">|</span>   <span class="n">id</span> <span class="o">|</span> <span class="n">first_name</span> <span class="o">|</span> <span class="n">last_name</span> <span class="o">|</span> <span class="n">biography</span> <span class="o">|</span>       <span class="n">registered</span> <span class="o">|</span>
<span class="o">+</span><span class="c1">----+------+------------+-----------+-----------+------------------+</span>
<span class="o">|</span> <span class="o">+</span><span class="n">I</span> <span class="o">|</span> <span class="mi">1010</span> <span class="o">|</span>       <span class="n">John</span> <span class="o">|</span>    <span class="n">Thomas</span> <span class="o">|</span> <span class="n">ZbJ0duDvW</span> <span class="o">|</span> <span class="mi">1741642600000000</span> <span class="o">|</span>
<span class="o">|</span> <span class="o">-</span><span class="n">U</span> <span class="o">|</span> <span class="mi">1010</span> <span class="o">|</span>       <span class="n">John</span> <span class="o">|</span>    <span class="n">Thomas</span> <span class="o">|</span> <span class="n">ZbJ0duDvW</span> <span class="o">|</span> <span class="mi">1741642600000000</span> <span class="o">|</span>
<span class="o">|</span> <span class="o">+</span><span class="n">U</span> <span class="o">|</span> <span class="mi">1010</span> <span class="o">|</span>       <span class="n">John</span> <span class="o">|</span>   <span class="n">Stenton</span> <span class="o">|</span> <span class="n">ZbJ0duDvW</span> <span class="o">|</span> <span class="mi">1741642600000000</span> <span class="o">|</span>
<span class="o">|</span> <span class="o">-</span><span class="n">D</span> <span class="o">|</span> <span class="mi">1010</span> <span class="o">|</span>       <span class="n">John</span> <span class="o">|</span>   <span class="n">Stenton</span> <span class="o">|</span> <span class="n">ZbJ0duDvW</span> <span class="o">|</span> <span class="mi">1741642600000000</span> <span class="o">|</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>For a source table it is typically not required to map the Kafka message key field(s) to the table schema when using the Debezium data format.
Instead, they are part of the change event value.
For situations where that’s not the case, key fields can be mapped via the <code>key.fields</code> configuration option;
also the <code>value.fields-include</code> option must be set to <code>EXCEPT_KEY</code> then.
Optionally, <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/debezium/#available-metadata">additional Debezium metadata fields</a> such as the origin timestamp or the name of the source table and schema can be mapped as virtual columns:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">authors_changelog_source</span> <span class="p">(</span>
  <span class="n">ts_ms</span> <span class="n">TIMESTAMP_LTZ</span> <span class="n">METADATA</span> <span class="k">FROM</span> <span class="s1">&#39;value.ingestion-timestamp&#39;</span> <span class="n">VIRTUAL</span><span class="p">,</span> <i class="conum" data-value="1"></i><b>(1)</b>
  <span class="n">source_table</span> <span class="n">STRING</span> <span class="n">METADATA</span> <span class="k">FROM</span> <span class="s1">&#39;value.source.table&#39;</span> <span class="n">VIRTUAL</span><span class="p">,</span>  <i class="conum" data-value="2"></i><b>(2)</b>
  <span class="n">source_properties</span> <span class="k">MAP</span><span class="o">&lt;</span><span class="n">STRING</span><span class="p">,</span> <span class="n">STRING</span><span class="o">&gt;</span> <span class="n">METADATA</span> <span class="k">FROM</span> <span class="s1">&#39;value.source.properties&#39;</span> <span class="n">VIRTUAL</span><span class="p">,</span>  <i class="conum" data-value="3"></i><b>(3)</b>
  <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="p">...</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="p">...</span>
<span class="p">);</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tbody><tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Maps the <code>ts_ms</code> field of the change events (the time at which the data change occurred in the source database)</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Maps the <code>source.table</code> field of the change events</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Maps all the <code>source</code> metadata of the change events</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>Flink’s Debezium data format requires change events to have not only the <code>after</code> section,
but also the <code>before</code> part which describes the previous state of a row which got updated or deleted.
This old row image is <a href="https://www.linkedin.com/feed/update/urn:li:activity:7305948780120453120/">required by Flink</a> for retracting previous values when incrementally re-computing derived data views.
Unfortunately, this means that Postgres users can leverage this format only for tables <a href="https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/formats/debezium/#consuming-data-produced-by-debezium-postgres-connector">which have a replica identity of <code>FULL</code></a>.
Otherwise, the old row image isn’t captured in the Postgres WAL and thus not exposed via logical replication.
An exception is raised in this case:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>java.lang.IllegalStateException: The &#34;before&#34; field of UPDATE message is null, if you are using Debezium Postgres Connector, please check the Postgres table has been set REPLICA IDENTITY to FULL level.
  at org.apache.flink.formats.json.debezium.DebeziumJsonDeserializationSchema.deserialize(DebeziumJsonDeserializationSchema.java:159)
  ...
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>While Flink’s <code>ChangelogNormalize</code> operator can materialize the retract events (at the cost of persisting all the required data in its own state store),
this currently is not supported when using the Apache Kafka SQL Connector as a changelog source with the Debezium change event format.
I don’t think there’s a fundamental issue which would prevent this from being possible,
it just currently isn’t implemented.</p>
</div>
<div class="paragraph">
<p>In order to propagate change events to another Kafka topic,
you’ll need to set up a sink connector, also using <code>debezium-json</code> as the value format.
You can define which field(s) should go into the Kafka message key via the <code>key.fields</code> property.
Make sure to use <code>json</code> (not <code>debezium-json</code>!) as the key format:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">authors_changelog_sink</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="n">first_name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">last_name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">biography</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">registered</span> <span class="nb">BIGINT</span>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">&#39;connector&#39;</span> <span class="o">=</span> <span class="s1">&#39;kafka&#39;</span><span class="p">,</span>
  <span class="s1">&#39;topic&#39;</span> <span class="o">=</span> <span class="s1">&#39;authors_processed&#39;</span><span class="p">,</span>
  <span class="s1">&#39;properties.bootstrap.servers&#39;</span> <span class="o">=</span> <span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span>
  <span class="s1">&#39;key.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;json&#39;</span><span class="p">,</span>
  <span class="s1">&#39;key.fields&#39;</span> <span class="o">=</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span>
  <span class="s1">&#39;value.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;debezium-json&#39;</span>
<span class="p">);</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>While the events on the downstream Kafka topic adhere to the Debezium’s event envelope schema,
they are produced by Flink, not Debezium.
In particular, they are lacking all the metadata you’d usually find in the <code>source</code> block.
Also updates are reflected by two events, rather than a single event as Debezium would emit it:
a deletion event with the old row state, followed by an insert event with the new row state.</p>
</div>
<div class="paragraph">
<p><em>When to use it:</em> The Apache Kafka SQL connector as a changelog source (and sink) is great when you want to implement streaming queries against incoming data change events,
for instance in order to create denormalized views or to enable real-time analytics of the data in an OLTP datastore.
It is not the best choice for ETL pipelines which don’t require stateful processing due to the removal of all the Debezium metadata.
Also, splitting updates into a delete and insert event causes write amplification in downstream systems,
which otherwise might support in-place updates to existing rows.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_upsert_kafka_sql_connector">The Upsert Kafka SQL Connector</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Last, let’s take a look at the Upsert Kafka SQL Connector.
It consumes/produces a changelog stream applying &#34;upsert&#34; semantics.
As a source connector, the first event for a given key is considered an <code>INSERT</code>,
all subsequent events for that key with a non-null value are considered <code>UPDATE</code>s to the same.
Tombstone records on the Kafka topic (i.e. records with a key and a null value) are interpreted as <code>DELETE</code> events for that key.</p>
</div>
<div class="admonitionblock note">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Tombstone records are used by Kafka to remove records during <a href="https://kafka.apache.org/documentation/#compaction">log compaction</a>.
You therefore need to configure a value for the topic’s <a href="https://kafka.apache.org/documentation/#topicconfigs_delete.retention.ms"><code>delete.retention.ms</code></a> setting which is long enough to make sure Flink gets to ingest all tombstones,
also considering there may be downtimes of your processing job.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>As a sink connector, any insert or update for a key yields an event with the current state as the value,
and the deletion of a key yields a tombstone record.</p>
</div>
<div class="paragraph">
<p>In order for Debezium to emit such a &#34;flat&#34; event structure with just the current state of a row—​instead of the full Debebezium change event envelope—​the
<a href="https://debezium.io/documentation/reference/stable/transformations/event-flattening.html">new record state transformation</a> (a Kafka Connect <a href="/blog/single-message-transforms-swiss-army-knife-of-kafka-connect/">single message transform</a>, SMT) needs to be applied when configuring the connector:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="json"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="p">{</span><span class="w">
  </span><span class="nl">&#34;name&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;inventory-connector&#34;</span><span class="p">,</span><span class="w">
  </span><span class="nl">&#34;config&#34;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">&#34;connector.class&#34;</span><span class="p">:</span><span class="w">
        </span><span class="s2">&#34;io.debezium.connector.postgresql.PostgresConnector&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;tasks.max&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;1&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;database.hostname&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;postgres&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;database.port&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;5432&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;database.user&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;postgres&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;database.password&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;postgres&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;database.dbname&#34;</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;postgres&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;topic.prefix&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;dbserver1&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;schema.include.list&#34;</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;inventory&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;slot.name&#34;</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;dbserver1&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;plugin.name&#34;</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;pgoutput&#34;</span><span class="p">,</span><span class="w">

    </span><span class="nl">&#34;transforms&#34;</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;unwrap&#34;</span><span class="p">,</span><span class="w"> <i class="conum" data-value="1"></i><b>(1)</b>
    </span><span class="nl">&#34;transforms.unwrap.type&#34;</span><span class="w"> </span><span class="p">:</span><span class="w">
        </span><span class="s2">&#34;io.debezium.transforms.ExtractNewRecordState&#34;</span><span class="p">,</span><span class="w">
    </span><span class="nl">&#34;transforms.unwrap.drop.tombstones&#34;</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;false&#34;</span><span class="w"> <i class="conum" data-value="2"></i><b>(2)</b>
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tbody><tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Apply the <code>ExtractNewRecordState</code> transform before sending the events to Kafka</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>As some Kafka Connect sink connectors can’t handle tombstone records, the connector supports dropping them. Setting this option will keep tombstone records, allowing to propagate delete events to Flink</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>With this SMT in place, the contents of the <code>after</code> section of <code>INSERT</code> and <code>UPDATE</code> events will be extracted and propagated as the sole change event value, i.e. the new row state.
<code>DELETE</code> events will be propagated as Kafka tombstones, as expected by the upsert connector.
Note that the <code>ExtractNewRecordState</code> SMT is <a href="https://debezium.io/documentation/reference/stable/transformations/event-flattening.html#configuration-options">highly configurable</a>, for instance you could opt into exporting specific <code>source</code> metadata properties as fields in the change event value, or as header properties of the emitted Kafka records.</p>
</div>
<div class="paragraph">
<p>The configuration of a source table for the upsert connector is pretty similar to the previous changelog source,
only that the connector type is <code>upsert-kafka</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="sql"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">authors_upsert_source</span> <span class="p">(</span>
  <span class="n">id</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="n">first_name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">last_name</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">biography</span> <span class="n">STRING</span><span class="p">,</span>
  <span class="n">registered</span> <span class="nb">BIGINT</span><span class="p">,</span>
  <span class="k">PRIMARY</span> <span class="k">KEY</span> <span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="k">NOT</span> <span class="n">ENFORCED</span> <i class="conum" data-value="1"></i><b>(1)</b>
<span class="p">)</span> <span class="k">WITH</span> <span class="p">(</span>
  <span class="s1">&#39;connector&#39;</span> <span class="o">=</span> <span class="s1">&#39;upsert-kafka&#39;</span><span class="p">,</span>
  <span class="s1">&#39;topic&#39;</span> <span class="o">=</span> <span class="s1">&#39;dbserver1.inventory.authors&#39;</span><span class="p">,</span>
  <span class="s1">&#39;properties.bootstrap.servers&#39;</span> <span class="o">=</span> <span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span>
  <span class="s1">&#39;key.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;json&#39;</span><span class="p">,</span>
  <span class="s1">&#39;value.format&#39;</span> <span class="o">=</span> <span class="s1">&#39;json&#39;</span>
<span class="p">);</span>
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tbody><tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>A primary key definition is mandatory when using the upsert connector; it determines which field(s) are part of the Kafka message key and thus are forming the upsert key</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>The same goes for defining sink tables.
Unfortunately, it is not possible to feed a changelog stream sourced from full Debezium change events (i.e. with the envelope) into an upsert sink,
as the <code>upsert-kafka</code> connector doesn’t support the <code>debezium-json</code> format as a source:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>ValidationException: &#39;upsert-kafka&#39; connector doesn&#39;t support
&#39;debezium-json&#39; as value format, because &#39;debezium-json&#39; is
not in insert-only mode.
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Neither can a changelog stream ingested via the non-upsert connector be propagated to the upsert connector:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre>UnsupportedOperationException: Unsupported to visit node
StreamPhysicalDropUpdateBefore. The node either should not be pushed
through the changelog normalize or is not supported yet.
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>Being able to do so could be pretty interesting for instance for writing updates to an incrementally recomputed materialized view to an OLAP store for serving purposes,
without incurring the overhead of the delete + insert event pair emitted by the non-upsert connector.
Maybe in a future Flink version?</p>
</div>
<div class="paragraph">
<p><em>When to use it:</em> Use the Upsert Kafka SQL Connector for processing &#34;flat&#34; Data change events, without the Debezium event envelope.
Similar to the Kafka SQL Connector as a changelog source, the upsert connector lets you implement streaming queries on change event feeds.
Unlike the Kafka SQL Connector, updates are emitted as a single event, which results in less write overhead on downstream systems,
in particular if partial updates (rather than full row rewrites) are supported.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When venturing into the world of processing Debezium data change events in realtime with Apache Flink and Flink SQL,
the combination of available connectors and data formats for doing so can be somewhat overwhelming.
The table below gives an overview over the different options, their characteristics, and use cases:</p>
</div>
<table class="tableblock frame-all grid-all stripes-even stretch">
<colgroup>
<col style="width: 18.1818%;"/>
<col style="width: 27.2727%;"/>
<col style="width: 27.2727%;"/>
<col style="width: 27.2728%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"><strong>Connector</strong></th>
<th class="tableblock halign-left valign-top"><strong>Kafka SQL Connector</strong></th>
<th class="tableblock halign-left valign-top"><strong>Kafka SQL Connector as changelog source</strong></th>
<th class="tableblock halign-left valign-top"><strong>Upsert Kafka SQL Connector</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Stream type</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Append-only</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Changelog</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Changelog</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Change event format</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>json</code>, <code>avro-confluent</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>debezium-json</code>, <code>debezium-avro-confluent</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>json</code>, <code>avro-confluent</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Input event type</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Debezium change event envelope</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Debezium change event envelope</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Flat events with current state; tombstone records</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Output event type</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Debezium change event envelope</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Synthetic Debezium change event envelope; updates broken up into delete + insert event</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Flat events with current state; tombstone records</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Metadata</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">In change event envelope</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mapped to table schema</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mapped to table schema, must be part of row state</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Start reading position</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Configurable</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Configurable</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Earliest offset</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>When to use</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Processing of change events themselves, e.g. transformation, enrichment, routing</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Realtime queries on changelog streams of full Debezium events, e.g. to create materialized views and enable realtime analytics</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Realtime queries on changelog streams of &#34;flat&#34; data change events, e.g. to create materialized views and enable realtime analytics</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Interestingly, whereas the Apache Flink project itself provides two separate Kafka connectors for upsert and non-upsert use cases,
managed Flink SQL offerings in the cloud tend to provide a more unified experience centered around one single higher-level connector.
As an example, the connector for integrating Flink with Kafka topics on Confluent Cloud exposes a setting <a href="https://docs.confluent.io/cloud/current/flink/reference/statements/create-table.html#flink-sql-create-table-with-changelog-mode"><code>changelog.mode</code></a>,
which defaults to <code>append</code> when deriving a Flink table from an uncompacted Kafka topic and to <code>upsert</code> for compacted topics.
Similar abstractions exist on other services too,
with the general aim being to shield users from some of the intricacies here.</p>
</div>
<div class="paragraph">
<p>One more thing you might wonder at this point is: how does <a href="https://nightlies.apache.org/flink/flink-cdc-docs-master/">Flink CDC</a> fit into all this?
Also hosted by the Apache Software Foundation,
this project integrates Debezium as a native connector into Flink,
instead of channeling data change events through Apache Kafka.
The Flink CDC connectors also emit changelog streams with retraction events as shown above,
only the Postgres connector optionally supports upsert semantics via its <a href="https://nightlies.apache.org/flink/flink-cdc-docs-master/docs/connectors/flink-sources/postgres-cdc/#connector-options"><code>changelog-mode</code></a> setting.</p>
</div>
<div class="paragraph">
<p>There are pros and cons for both ways of integrating Debezium and Flink,
for instance in regards to the replayability of events.
This warrants a separate blog post just dedicated to comparing both approaches at some point, though.</p>
</div>
</div>
</div>
			</div>

			<div class="post-tags">
				
					
				
			</div>
		</div><div id="disqus_thread">
  <script src="https://giscus.app/client.js"
    data-repo="gunnarmorling/discussions.morling.dev"
    data-repo-id="R_kgDOGXzqNQ"
    data-category="Announcements"
    data-category-id="DIC_kwDOGXzqNc4B_2Pq"
    data-mapping="title"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-theme="light"
    data-lang="en"
    crossorigin="anonymous"
    async>
  </script>
</div>

<noscript>Please enable JavaScript, or join the <a href="https://github.com/gunnarmorling/discussions.morling.dev/discussions/">discussion on GitHub</a>.</noscript>
</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> © 2019 - 2025 Gunnar Morling |  Licensed Under <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons BY-SA 4.0</a></div>
	</nav>
</div><script>
	mediumZoom(document.querySelectorAll('div.imageblock > div.content > img'))
</script>

</body>
</html>
