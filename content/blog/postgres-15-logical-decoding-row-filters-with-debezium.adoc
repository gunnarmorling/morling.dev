---
title: 'Postgres 15: Logical Decoding Row Filters With Debezium'
date: "2022-12-15T00:00:00+00:00"
draft: true
markup: adoc
canonical_url: https://www.decodable.co/blog/postgres-15-logical-decoding-row-filters-with-debezium
tags:
  - postgres
  - debezium
  - cdc
---
:source-highlighter: rouge
:rouge-style: base16.dark
:icons: font
:imagesdir: /images
ifdef::env-github[]
:imagesdir: ../../static/images
endif::[]

_This post originally appeared on the link:https://www.decodable.co/blog/postgres-15-logical-decoding-row-filters-with-debezium[Decodable blog]._

Since link:https://www.postgresql.org/docs/current/logicaldecoding-explanation.html[logical decoding] was added to Postgres in version 9.4, this powerful feature for capturing changes from the write-ahead log of the database has been continuously improved.
link:https://www.postgresql.org/about/news/postgresql-15-released-2526/[Postgres 15], released in October this year, added support for fine-grained control over which columns (by means of _column lists_) and rows (via _row filters_) should be exported from captured tables.
This means, in relational terminology, projections and filters are now natively supported by Postgres change event publications.

<!--more-->
Reasons for specifically configuring which columns and rows should be contained in a change data stream are manifold:

* Excluding large columns (say, a binary column with image data) can significantly reduce the size of change events and thus the required network bandwidth
* Excluding columns or rows with sensitive data can be necessary in order to satisfy privacy requirements, when for instance Personally Identifiable Information (PII) shouldn't be exposed to external systems
* Filtering published rows by tenant id can be useful for setting up tenant-specific change streams in a multi-tenant architecture

Before the advent of Postgres-native column lists and row filters, users of link:https://debezium.io/[Debezium] -- a popular open-source platform for change data capture (CDC), which also is used by several Decodable CDC connectors -- would typically have implemented these kinds of use cases via a combination of configuration options and single message transformations (SMTs).

Projections are supported in Debezium via the link:https://debezium.io/documentation/reference/stable/connectors/postgresql#postgresql-property-column-include-list[column.include.list] and link:https://debezium.io/documentation/reference/stable/connectors/postgresql#postgresql-property-column-exclude-list[column.exclude.list] options.
These configuration options are applied client-side, i.e.
within the Debezium connector, which makes them less efficient to server-side column lists, potentially causing large amounts of data to be streamed to Debezium, only to be discarded there.

Filters are a bit more involved: while there is built-in support for filtering the contents of link:https://debezium.io/documentation/reference/stable/connectors/postgresql#postgresql-property-snapshot-select-statement-overrides[initial] and ad-hoc link:https://debezium.io/documentation/reference/stable/connectors/postgresql#postgresql-ad-hoc-snapshots[incremental snapshots], filtering change events emitted from the WAL requires a custom SMT.
Pushing this logic into the logical replication mechanism of the database itself makes a lot of sense from a usability and efficiency perspective.

So let's see how Postgres 15 row filters can be used together with Debezium.
Initially, I meant to demonstrate the usage of column lists, too.
But in the course of exploring that feature, I link:https://www.postgresql.org/message-id/flat/CADGJaX9kiRZ-OH0EpWF5Fkyh1ZZYofoNRCrhapBfdk02tj5EKg%40mail.gmail.com[discovered a bug] in Postgres which causes incorrect events to be emitted for UPDATE and DELETE statements when column lists are present.
So this will have to wait for another time.
The Postgres community took care of this super fast: a bug fix has already been applied, so that column lists should work as expected in the next Postgres release.


== Using Logical Decoding Row Filters With Debezium
To follow along, check out the link:https://github.com/gunnarmorling/postgres-publication-filtering[postgres-publication-filtering] demo project from GitHub.
It contains a Docker Compose file for running Postgres as well as Apache Kafka and Kafka Connect with Debezium:


[source,bash,linenums]
----
git clone git@github.com:gunnarmorling/postgres-publication-filtering.git
cd postgres-publication-filtering
docker-compose up --build
----


That Postgres example container image contains a table products with the following schema:


image::postgres15-row-filters-1.webp[]
Let's set up a change event stream for that table which only contains events if the quantity of the given product item is below 10.
We could then for instance envision a microservice which subscribes to that stream and places backfill orders with our suppliers for those products.

Row filters are configured via Postgres link:https://www.postgresql.org/docs/current/logical-replication-publication.html[publications], as used with the link:https://www.postgresql.org/docs/current/protocol-logicalrep-message-formats.html[pgoutput] logical decoding plug-in.
As Debezium can only create publications with the default settings (at least for now), you need to manually create a custom publication with the required configurations and have Debezium make use of it.
To do so, launch a Postgres session via pgcli:


[source,bash,linenums]
----
docker run --tty --rm -i \
   --network postgres-publication-filtering_default \
   quay.io/debezium/tooling:1.2 \
   bash -c 'pgcli postgresql://postgresuser:postgrespw@postgres:5432/postgresdb'
----


Then create a publication like so:


[source,sql,linenums]
----
SET search_path TO inventory;
CREATE PUBLICATION inventory_publication FOR TABLE products WHERE (quantity < 10);
----


As of Postgres 15, the CREATE PUBLICATION statement allows you to narrow down the events to be emitted for a given table via a custom WHERE clause.
A few conditions apply to that clause (see link:http://amitkapila16.blogspot.com/2022/11/logical-replication-improvements-in.html[this post] for more information), most importantly:

* If the publication publishes UPDATE or DELETE events, only columns which are part of the table's link:https://debezium.io/documentation/reference/stable/connectors/postgresql#postgresql-replica-identity[replica identity] may be referenced
* Only simple expressions are allowed, for example not referring to user-defined functions or types, system columns etc.

That's all we need to do on the Postgres side.
Now let's take a look at the required Debezium configuration:


[source,json,linenums]
----
"plugin.name" : "pgoutput",
"publication.autocreate.mode" : "disabled",
"publication.name" : "inventory_publication",
----


As the Postgres publication only is used when Debezium retrieves change events via logical decoding from the WAL, you also need to customize the SELECT statement used for the products table when snapshotting the table.
Otherwise, you'd get snapshot events for _all_ the rows of that table, no matter what their quantity is.
This can be done via the following configuration:


[source,json,linenums]
----
"snapshot.select.statement.overrides" : "inventory.products:WHERE quantity < 10"
----


Altogether, the connector configuration looks like this:


[source,json,linenums]
----
{
  "name": "inventory-connector",
  "config": {
    "connector.class":"io.debezium.connector.postgresql.PostgresConnector",
    "tasks.max": "1",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "postgresuser",
    "database.password": "postgrespw",
    "database.dbname" : "postgresdb",
    "topic.prefix": "dbserver1",
    "schema.include.list": "inventory",
    "table.include.list" : "inventory.products,inventory.customers,inventory.test",
    "plugin.name" : "pgoutput",
    "publication.autocreate.mode" : "disabled",
    "publication.name" : "inventory_publication",
    "snapshot.select.statement.overrides" : "inventory.products",
    "snapshot.select.statement.overrides.inventory.products" :
        "SELECT * FROM inventory.products WHERE quantity < 10"
  }
}
----


Now register a connector instance with this configuration.
If you have link:https://github.com/kcctl/kcctl[kcctl] ðŸ§¸ installed (which I highly recommend), that's as simple as that:


[source,bash,linenums]
----
kcctl apply -f inventory-connector.json
----


Alternatively, use curl to post the configuration directly to Kafka Connect's REST API:


[source,bash,linenums]
----
curl -i -X POST -H "Accept:application/json" -H"Content-Type:application/json" \
    http://localhost:8083/connectors/ -d @inventory-connector.json
----

== Observing Filtered Change Events
Return to your Postgres session and display the contents of the products table:


[source,sql,linenums]
----
postgresdb> select id, name, quantity from inventory.products;
+------+--------------------+------------+
| id   | name               | quantity   |
|------+--------------------+------------|
| 101  | scooter            | 5          |
| 102  | car battery        | 10         |
| 103  | 12-pack drill bits | 44         |
| 104  | hammer             | 12         |
| 105  | hammer             | 42         |
| 106  | hammer             | 37         |
| 107  | rocks              | 9          |
| 108  | jacket             | 19         |
| 109  | spare tire         | 28         |
+------+--------------------+------------+
SELECT 9
Time: 0.050s
----


Out of those nine product items, only those with a quantity of less than ten show up as snapshot events in the corresponding Kafka topic:


[source,bash,linenums]
----
docker run --tty --rm \
  --network postgres-publication-filtering_default \
  quay.io/debezium/tooling:1.2 \
  kafkacat -b kafka:9092 -C -o beginning -q \
  -t dbserver1.inventory.products | jq '.payload | { op, ts_ms, after }'
----

[source,json,linenums]
----
{
  "op": "r",
  "ts_ms": 1669375471236,
  "after": {
    "id": 101,
    "name": "scooter",
    "description": "Small 2-wheel scooter",
    "weight": 3.14,
    "quantity": 5
  }
}
{
  "op": "r",
  "ts_ms": 1669375471238,
  "after": {
    "id": 107,
    "name": "rocks",
    "description": "box of assorted rocks",
    "weight": 5.3,
    "quantity": 9
  }
}
----


Now let's do some data changes and observe the resulting change events, as retrieved from the database via logical decoding.
First, insert a few records into the table:


[source,sql,linenums]
----
INSERT INTO products
VALUES (DEFAULT, 'deck chair', 'A cozy wooden deck chair', 15.7, 7),
			(DEFAULT, 'paint', 'A bucket of white paint', 5.0, 15),
			(DEFAULT, 'lamp', 'A green library style lamp', 4.8, 3);
----


Nothing too exciting is happening in the Kafka topic: as you would expect, only events for the deck chair and the lamp products show up in Kafka, but not for the paint item, as its quantity is larger than 10.
Things get a bit more interesting when doing some updates:


[source,sql,linenums]
----
UPDATE products SET quantity = 6 WHERE NAME = 'deck chair';
UPDATE products SET quantity = 14 WHERE NAME = 'paint';
UPDATE products SET quantity = 9 WHERE NAME = 'paint';
UPDATE products SET quantity = 11 WHERE NAME = 'lamp';
----


The following three events are emitted to Kafka for those:


[source,json,linenums]
----
{
  "op": "u",
  "ts_ms": 1669382021989,
  "before": {
    "id": 110,
    "name": "deck chair",
    "description": "A cozy wooden deck chair",
    "weight": 15.7,
    "quantity": 7
   },
  "after": 
   {
     "id": 110,
    "name": "deck chair",
    "description": "A cozy wooden deck chair",
    "weight": 15.7,
  "quantity": 6
   }
}
{
  "op": "c",
  "ts_ms": 1669382021990,
  "before": null,
  "after": 
  {
    "id": 111,
    "name": "paint",
    "description": "A bucket of white paint",
    "weight": 5,
    "quantity": 9
  }
}
{
  "op": "d",
  "ts_ms": 1669382021990,
  "before": 
  {
    "id": 112,
    "name": "lamp",
    "description": "A green library style lamp",
    "weight": 4.8,
    "quantity": 3
  },
"after": null
}
----


Note not all of them have the u (update) operation type, but some are c (create) and d (delete) events.
The logic here is that the publication works from a perspective of looking at the row set specified via the WHERE clause for the table.
In that light,

* An update event is emitted for the deck chair quantity update from 7 to 6
* No event event is emitted for the paint quantity update from 15 to 14, as that row is not part of this row set before and after the change
* A create event is emitted for the paint quantity update from 14 to 9, as that row now became a part of the row set
* A delete event is emitted for the lamp quantity update from 3 to 11, as that row now is not a part of the row set any longer

Finally, let's delete some product items:


[source,sql,linenums]
----
DELETE FROM products WHERE NAME = 'lamp';
DELETE FROM products WHERE NAME = 'deck chair';
----


In the Kafka topic you can observe that no change event is emitted for the first deletion (as there's 11 lamps in stock).
But there is an event for the deletion of the deck chair record with a quantity of six.

As they say, a picture is worth a thousand words (and I'd never pass on an opportunity for using my favorite tool link:https://excalidraw.com/[Excalidraw] ), so here is an overview of the published events, depending on the specifics of a given data change:


image::postgres15-row-filters-2.webp[]

== Wrap-Up
Row filters (and column lists) are a great addition to the Postgres logical decoding toolbox.
Having fine-grained control over which change events should be published and which field they should contain, opens up many interesting opportunities from a perspective of efficiency and data privacy as well as the ability to set up content specific change data streams, as demonstrated in the example above.

Going forward, a good next step usability-wise would be for Debezium to apply any configured row filters and column lists to the Postgres publications it creates, simplifying things for users a bit.
As far as Flink SQL and Decodable are concerned, row filters and column lists potentially allow for the _push down_ of filter and projection operators of link:https://docs.decodable.co/docs/sqlpipeline-reference[streaming SQL queries] ; Instead of applying these operators within the Flink stream processing engine, SELECT and WHERE clauses of queries could be re-written transparently and these operators executed as part of the logical replication publication within Postgres itself.
Flink supports this kind of push down of logic into data sources via the link:https://nightlies.apache.org/flink/flink-docs-release-1.16/api/java/org/apache/flink/table/connector/source/abilities/SupportsFilterPushDown.html[SupportsFilterPushDown] and link:https://nightlies.apache.org/flink/flink-docs-release-1.16/api/java/org/apache/flink/table/connector/source/abilities/SupportsProjectionPushDown.html[SupportsProjectionPushDown] extension points.
For example, this could be very interesting to customers who don't want specific segments of their data to leave the realm of their database.
Please reach out to us if you think this would be an interesting capability to have.

If you would like to get started with your own experimentations around 15 Postgres row filters using Debezium, you can find the complete source code of the example shown above in link:https://github.com/gunnarmorling/postgres-publication-filtering[this repository] on GitHub.
You can find more information about row filters in link:https://www.postgresql.fastware.com/blog/introducing-publication-row-filters[this blog post] ; also refer to link:http://amitkapila16.blogspot.com/2022/11/logical-replication-improvements-in.html[this post] to learn more about this and other new features related to logical replication in Postgres 15.

_Many thanks to link:https://twitter.com/rmetzger$$_$$[Robert Metzger] for his feedback while writing this post!_
