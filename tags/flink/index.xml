<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>flink on Gunnar Morling</title>
    <link>https://www.morling.dev/tags/flink/</link>
    <description>Recent content in flink on Gunnar Morling</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2019 - 2025 Gunnar Morling</copyright>
    <lastBuildDate>Wed, 18 Jun 2025 15:30:00 +0200</lastBuildDate>
    <atom:link href="https://www.morling.dev/tags/flink/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>This AI Agent Should Have Been a SQL Query</title>
      <link>https://www.morling.dev/blog/this-ai-agent-should-have-been-sql-query/</link>
      <pubDate>Wed, 18 Jun 2025 15:30:00 +0200</pubDate>
      <guid>https://www.morling.dev/blog/this-ai-agent-should-have-been-sql-query/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;&#xA;&lt;div id=&#34;toctitle&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul class=&#34;sectlevel1&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_agents_need_to_interact_with_llms&#34;&gt;Agents Need to Interact With LLMs&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_agents_should_be_event_driven&#34;&gt;Agents Should Be Event-Driven&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_agents_need_context&#34;&gt;Agents Need Context&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_agents_require_memory&#34;&gt;Agents Require Memory&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_when_sql_is_not_enough&#34;&gt;When SQL Is Not Enough&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_parting_thoughts&#34;&gt;Parting Thoughts&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph teaser&#34;&gt;&#xA;&lt;p&gt;AI Agents have improved in leaps and bounds in recent times, moving beyond simple chatbots to sophisticated, autonomous systems. This post explores a novel approach to building agentic systems: using the power of streaming SQL queries. Discover how platforms like Apache Flink can transform the development of AI Agents, offering benefits in consistency, scalability, and developer experience.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Backfilling Postgres TOAST Columns in Debezium Data Change Events</title>
      <link>https://www.morling.dev/blog/backfilling-postgres-toast-columns-debezium-change-events/</link>
      <pubDate>Mon, 26 May 2025 16:40:00 +0200</pubDate>
      <guid>https://www.morling.dev/blog/backfilling-postgres-toast-columns-debezium-change-events/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;&#xA;&lt;div id=&#34;toctitle&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul class=&#34;sectlevel1&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_debezium_reselect_postprocessor&#34;&gt;Debezium Reselect Postprocessor&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_flink_datastream_api&#34;&gt;Flink DataStream API&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_flink_sql_with_over_aggregation&#34;&gt;Flink SQL With OVER Aggregation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_flink_process_table_functions&#34;&gt;Flink Process Table Functions&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_summary_and_discussion&#34;&gt;Summary and Discussion&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph teaser&#34;&gt;&#xA;&lt;p&gt;Postgres logical replication, while powerful for capturing real-time data changes, presents challenges with TOAST columns,&#xA;whose values can be absent from data change events in specific situations.&#xA;This post discusses how Debezium addresses this through its built-in reselect post processor,&#xA;then explores more robust solutions leveraging Apache Flink’s capabilities for stateful stream processing,&#xA;including Flink SQL and the brand-new process table functions (PTFs) in Flink 2.1.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>A Deep Dive Into Ingesting Debezium Events From Kafka With Flink SQL</title>
      <link>https://www.morling.dev/blog/ingesting-debezium-events-from-kafka-with-flink-sql/</link>
      <pubDate>Wed, 16 Apr 2025 11:25:00 +0200</pubDate>
      <guid>https://www.morling.dev/blog/ingesting-debezium-events-from-kafka-with-flink-sql/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;&#xA;&lt;div id=&#34;toctitle&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul class=&#34;sectlevel1&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_flink_sql_connectors_for_apache_kafka&#34;&gt;Flink SQL Connectors for Apache Kafka&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_the_apache_kafka_sql_connector_in_append_only_mode&#34;&gt;The Apache Kafka SQL Connector in Append-Only Mode&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_the_apache_kafka_sql_connector_as_a_changelog_source&#34;&gt;The Apache Kafka SQL Connector As a Changelog Source&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_the_upsert_kafka_sql_connector&#34;&gt;The Upsert Kafka SQL Connector&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Over the years, I’ve spoken quite a bit about the use cases for processing &lt;a href=&#34;https://2023.javazone.no/program/355869fa-5aa0-43a7-abd2-7c5250e10bcd&#34;&gt;Debezium data change events with Apache Flink&lt;/a&gt;,&#xA;such as metadata enrichment, building denormalized data views, and creating data contracts for your CDC streams.&#xA;One detail I haven’t covered in depth so far is how to actually ingest Debezium change events from a Kafka topic into Flink,&#xA;in particular via Flink SQL.&#xA;Several connectors and data formats exist for this, which can make things somewhat confusing at first.&#xA;So let’s dive into the different options and the considerations around them!&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Get Running with Apache Flink on Kubernetes, part 2 of 2</title>
      <link>https://www.morling.dev/blog/get-running-with-apache-flink-on-kubernetes-2/</link>
      <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://www.morling.dev/blog/get-running-with-apache-flink-on-kubernetes-2/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;&#xA;&lt;div id=&#34;toctitle&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul class=&#34;sectlevel1&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_fault_tolerance_and_high_availability&#34;&gt;Fault Tolerance and High Availability&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_manually_triggering_savepoints&#34;&gt;Manually Triggering Savepoints&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_observability&#34;&gt;Observability&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_bonus_managing_flink_jobs_with_the_heimdall_ui&#34;&gt;Bonus: Managing Flink Jobs With the Heimdall UI&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_summary_and_discussion&#34;&gt;Summary and Discussion&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&#34;https://www.decodable.co/blog/get-running-with-apache-flink-on-kubernetes-2&#34;&gt;Decodable blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Welcome back to this two-part blog post series about running Apache Flink on Kubernetes, using the Flink Kubernetes operator.&#xA;In &lt;a href=&#34;https://www.morling.dev/blog/get-running-with-apache-flink-on-kubernetes-1&#34;&gt;part one&lt;/a&gt;, we discussed installation and setup of the operator, different deployment types, how to deploy Flink jobs using custom Kubernetes resources, and how to create container images for your own Flink jobs.&#xA;In this part, we’ll focus on aspects such as fault tolerance and high availability of your Flink jobs running on Kubernetes, savepoint management, observability, and more.&#xA;You can find the complete source code for all the examples shown in this series in the Decodable &lt;a href=&#34;https://github.com/decodableco/examples/blob/main/flink-on-kubernetes/&#34;&gt;examples repository&lt;/a&gt; on GitHub: on GitHub.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Get Running with Apache Flink on Kubernetes, part 1 of 2</title>
      <link>https://www.morling.dev/blog/get-running-with-apache-flink-on-kubernetes-1/</link>
      <pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://www.morling.dev/blog/get-running-with-apache-flink-on-kubernetes-1/</guid>
      <description>&lt;div id=&#34;toc&#34; class=&#34;toc&#34;&gt;&#xA;&lt;div id=&#34;toctitle&#34;&gt;Table of Contents&lt;/div&gt;&#xA;&lt;ul class=&#34;sectlevel1&#34;&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_installation_and_setup&#34;&gt;Installation and Setup&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_deployment_types&#34;&gt;Deployment Types&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_deploying_your_first_flink_job_on_kubernetes&#34;&gt;Deploying Your First Flink Job on Kubernetes&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;#_building_custom_job_images&#34;&gt;Building Custom Job Images&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&#34;https://www.decodable.co/blog/get-running-with-apache-flink-on-kubernetes-1&#34;&gt;Decodable blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/div&gt;&#xA;&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;Kubernetes is a widely used deployment platform for Apache Flink.&#xA;While Flink has had native support for Kubernetes for quite a while, it is in particular the operator pattern which makes deploying Flink jobs onto Kubernetes clusters a compelling option: you define jobs in a declarative resource, and a control loop running in a component called a Kubernetes operator takes care of provisioning and maintaining (e.g.&#xA;scaling, updating) all the required resources.&#xA;Automation is the keyword here, significantly reducing the manual effort required for running Flink jobs in production.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Maven, What Are You Waiting For?!</title>
      <link>https://www.morling.dev/blog/maven-what-are-you-waiting-for/</link>
      <pubDate>Sun, 18 Dec 2022 13:45:00 +0100</pubDate>
      <guid>https://www.morling.dev/blog/maven-what-are-you-waiting-for/</guid>
      <description>&lt;div class=&#34;paragraph&#34;&gt;&#xA;&lt;p&gt;As part of my &lt;a href=&#34;https://www.morling.dev/blog/why-i-joined-decodable/&#34;&gt;new job&lt;/a&gt; at Decodable,&#xA;I am also planning to contribute to the &lt;a href=&#34;https://flink.apache.org/&#34;&gt;Apache Flink&lt;/a&gt; project&#xA;(as Decodable’s fully-managed &lt;a href=&#34;https://www.decodable.co/product&#34;&gt;stream processing platform&lt;/a&gt; is based on Flink).&#xA;Right now, I am in the process of familiarizing myself with the Flink code base,&#xA;and as such I am of course building the project from source, too.&lt;/p&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Why I Joined Decodable</title>
      <link>https://www.morling.dev/blog/why-i-joined-decodable/</link>
      <pubDate>Thu, 03 Nov 2022 15:00:00 +0100</pubDate>
      <guid>https://www.morling.dev/blog/why-i-joined-decodable/</guid>
      <description>Table of Contents The Space: Real-Time Stream Processing The Environment: A Start-up The Team: One of a Kind Outlook It’s my first week as a software engineer at Decodable, a start-up building a serverless real-time data platform! When I shared this news on social media yesterday, folks were not only super supportive and excited for me (thank you so much for all the nice words and wishes!), but some also asked about the reasons behind my decision for switching jobs and going to a start-up, after having worked for Red Hat for the last few years.</description>
    </item>
  </channel>
</rss>
